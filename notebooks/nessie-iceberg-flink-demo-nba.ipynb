{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nessie Iceberg/Flink SQL Demo with NBA Dataset\n",
        "============================\n",
        "This demo showcases how to use Nessie Python API along with Flink from Iceberg\n",
        "\n",
        "Initialize PyFlink\n",
        "----------------------------------------------\n",
        "To get started, we will first have to do a few setup steps that give us everything we need\n",
        "to get started with Nessie. In case you're interested in the detailed setup steps for Flink, you can check out the [docs](https://projectnessie.org/tools/iceberg/flink/)\n",
        "\n",
        "The Binder server has downloaded flink and some data for us as well as started a Nessie server in the background. All we have to do is start Flink\n",
        "\n",
        "The below cell starts a local Flink session with parameters needed to configure Nessie. Each config option is followed by a comment explaining its purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "SLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/srv/conda/envs/flink-demo/lib/python3.7/site-packages/pyflink/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/srv/conda/envs/flink-demo/lib/python3.7/site-packages/pyflink/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n\n\nFlink running\n\n\n\n"
        }
      ],
      "source": [
        "import os\n",
        "from pyflink.datastream import StreamExecutionEnvironment\n",
        "from pyflink.table import StreamTableEnvironment\n",
        "from pyflink.table.expressions import lit\n",
        "from pynessie import init\n",
        "\n",
        "# where we will store our data\n",
        "warehouse = os.path.join(os.getcwd(), \"flink-warehouse\")\n",
        "# this was downloaded when Binder started, its available on maven central\n",
        "iceberg_flink_runtime_jar = os.path.join(os.getcwd(), \"../iceberg-flink-runtime-0.12.0.jar\")\n",
        "\n",
        "env = StreamExecutionEnvironment.get_execution_environment()\n",
        "env.add_jars(\"file://{}\".format(iceberg_flink_runtime_jar))\n",
        "table_env = StreamTableEnvironment.create(env)\n",
        "\n",
        "nessie_client = init()\n",
        "\n",
        "def create_ref_catalog(ref):\n",
        "    \"\"\"\n",
        "    Create a flink catalog that is tied to a specific ref.\n",
        "\n",
        "    In order to create the catalog we have to first create the branch\n",
        "    \"\"\"\n",
        "    hash_ = nessie_client.get_reference(nessie_client.get_default_branch()).hash_\n",
        "    try:\n",
        "        nessie_client.create_branch(ref, hash_)\n",
        "    except:\n",
        "        pass # already created\n",
        "    # The important args below are:\n",
        "    # type: tell Flink to use Iceberg as the catalog\n",
        "    # catalog-impl: which Iceberg catalog to use, in this case we want Nessie\n",
        "    # uri: the location of the nessie server.\n",
        "    # ref: the Nessie ref/branch we want to use (defaults to main)\n",
        "    # warehouse: the location this catalog should store its data\n",
        "    table_env.execute_sql(\n",
        "            f\"\"\"CREATE CATALOG {ref}_catalog WITH (\n",
        "            'type'='iceberg',\n",
        "            'catalog-impl'='org.apache.iceberg.nessie.NessieCatalog',\n",
        "            'uri'='http://localhost:19120/api/v1',\n",
        "            'ref'='{ref}',\n",
        "            'warehouse' = '{warehouse}')\"\"\"\n",
        "        )\n",
        "create_ref_catalog(nessie_client.get_default_branch())\n",
        "print(\"\\n\\n\\nFlink running\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Solving Data Engineering problems with Nessie\n",
        "============================\n",
        "\n",
        "In this Demo we are a data engineer working at a fictional sports analytics blog. In order for the authors to write articles they have to have access to the relevant data. They need to be able to retrieve data quickly and be able to create charts with it.\n",
        "\n",
        "We have been asked to collect and expose some information about basketball players. We have located some data sources and are now ready to start ingesting data into our data lakehouse. We will perform the ingestion steps on a Nessie branch to test and validate the data before exposing to the analysts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up Nessie branches (via Nessie CLI)\n",
        "----------------------------\n",
        "Once all dependencies are configured, we can get started with ingesting our basketball data into `Nessie` with the following steps:\n",
        "\n",
        "- Create a new branch named `dev`\n",
        "- List all branches\n",
        "\n",
        "It is worth mentioning that we don't have to explicitly create a `main` branch, since it's the default branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "create_ref_catalog(\"dev\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We have created the branch `dev` and we can see the branch with the Nessie `hash` its currently pointing to.\n",
        "\n",
        "Below we list all branches. Note that the auto created `main` branch already exists and both branches point at the same `hash`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\u001b[33m* main  2e1cfa82b035c26cbbbdae632cea070514eb8b773f616aaeaf668e2f0be8f10d comment\n\u001b[0m  dev   2e1cfa82b035c26cbbbdae632cea070514eb8b773f616aaeaf668e2f0be8f10d comment\n\n"
        }
      ],
      "source": [
        "!nessie --verbose branch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create tables under dev branch\n",
        "-------------------------------------\n",
        "Once we created the `dev` branch and verified that it exists, we can create some tables and add some data.\n",
        "\n",
        "We create two tables under the `dev` branch:\n",
        "- `salaries`\n",
        "- `totals_stats`\n",
        "\n",
        "These tables list the salaries per player per year and their stats per year.\n",
        "\n",
        "To create the data we:\n",
        "\n",
        "1. switch our branch context to dev\n",
        "2. create the table\n",
        "3. insert the data from an existing csv file. This csv file is already stored locally on the demo machine. A production use case would likely take feeds from official data sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "WARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/srv/conda/envs/flink-demo/lib/python3.7/site-packages/pyflink/lib/hadoop-auth-2.10.1.jar) to method sun.security.krb5.Config.getInstance()\nWARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2022-03-10 17:08:05,907 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:05,909 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:05,909 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:05,907 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:05,907 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:05,907 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:05,910 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:05,907 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:07,727 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:07,728 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:07,729 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:07,729 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:07,733 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:07,733 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:07,733 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:07,733 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n\n\n\nAdded 51 rows to the salaries table and 93 rows to the totals_stats table.\n\n\n\n"
        }
      ],
      "source": [
        "# Load the dataset\n",
        "from pyflink.table import DataTypes\n",
        "from pyflink.table.descriptors import Schema, OldCsv, FileSystem\n",
        "\n",
        "# Creating `salaries` table\n",
        "(table_env.connect(FileSystem().path('../datasets/nba/salaries.csv'))\n",
        "  .with_format(OldCsv()\n",
        "               .field('Season', DataTypes.STRING()).field(\"Team\", DataTypes.STRING())\n",
        "               .field(\"Salary\", DataTypes.STRING()).field(\"Player\", DataTypes.STRING()))\n",
        "  .with_schema(Schema()\n",
        "               .field('Season', DataTypes.STRING()).field(\"Team\", DataTypes.STRING())\n",
        "               .field(\"Salary\", DataTypes.STRING()).field(\"Player\", DataTypes.STRING()))\n",
        "  .create_temporary_table('dev_catalog.nba.salaries_temp'))\n",
        "\n",
        "table_env.execute_sql(\"\"\"CREATE TABLE IF NOT EXISTS dev_catalog.nba.salaries\n",
        "            (Season STRING, Team STRING, Salary STRING, Player STRING)\"\"\").wait()\n",
        "\n",
        "tab = table_env.from_path('dev_catalog.nba.salaries_temp')\n",
        "tab.execute_insert('dev_catalog.nba.salaries').wait()\n",
        "\n",
        "# Creating `totals_stats` table\n",
        "(table_env.connect(FileSystem().path('../datasets/nba/totals_stats.csv'))\n",
        "  .with_format(OldCsv()\n",
        "               .field('Season', DataTypes.STRING()).field(\"Age\", DataTypes.STRING()).field(\"Team\", DataTypes.STRING())\n",
        "               .field(\"ORB\", DataTypes.STRING()).field(\"DRB\", DataTypes.STRING()).field(\"TRB\", DataTypes.STRING())\n",
        "               .field(\"AST\", DataTypes.STRING()).field(\"STL\", DataTypes.STRING()).field(\"BLK\", DataTypes.STRING())\n",
        "               .field(\"TOV\", DataTypes.STRING()).field(\"PTS\", DataTypes.STRING()).field(\"Player\", DataTypes.STRING())\n",
        "               .field(\"RSorPO\", DataTypes.STRING()))\n",
        "  .with_schema(Schema()\n",
        "               .field('Season', DataTypes.STRING()).field(\"Age\", DataTypes.STRING()).field(\"Team\", DataTypes.STRING())\n",
        "               .field(\"ORB\", DataTypes.STRING()).field(\"DRB\", DataTypes.STRING()).field(\"TRB\", DataTypes.STRING())\n",
        "               .field(\"AST\", DataTypes.STRING()).field(\"STL\", DataTypes.STRING()).field(\"BLK\", DataTypes.STRING())\n",
        "               .field(\"TOV\", DataTypes.STRING()).field(\"PTS\", DataTypes.STRING()).field(\"Player\", DataTypes.STRING())\n",
        "               .field(\"RSorPO\", DataTypes.STRING()))\n",
        "  .create_temporary_table('dev_catalog.nba.totals_stats_temp'))\n",
        "\n",
        "table_env.execute_sql(\n",
        "        \"\"\"CREATE TABLE IF NOT EXISTS dev_catalog.nba.totals_stats (Season STRING, Age STRING, Team STRING,\n",
        "        ORB STRING, DRB STRING, TRB STRING, AST STRING, STL STRING, BLK STRING, TOV STRING, PTS STRING,\n",
        "        Player STRING, RSorPO STRING)\"\"\").wait()\n",
        "\n",
        "tab = table_env.from_path('dev_catalog.nba.totals_stats_temp')\n",
        "tab.execute_insert('dev_catalog.nba.totals_stats').wait()\n",
        "\n",
        "salaries = table_env.from_path('main_catalog.nba.`salaries@dev`').select(lit(1).count).to_pandas().values[0][0]\n",
        "totals_stats = table_env.from_path('main_catalog.nba.`totals_stats@dev`').select(lit(1).count).to_pandas().values[0][0]\n",
        "print(f\"\\n\\n\\nAdded {salaries} rows to the salaries table and {totals_stats} rows to the totals_stats table.\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now we count the rows in our tables to ensure they are the same number as the csv files. Note we use the `table@branch` notation which overrides the context set by the catalog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2022-03-10 17:08:14,552 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:14,625 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:14,628 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:14,631 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:14,633 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:14,636 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:14,638 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:14,640 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n51\n2022-03-10 17:08:16,530 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:16,620 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:16,624 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:16,628 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:16,631 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:16,634 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:16,638 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:16,641 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n93\n"
        }
      ],
      "source": [
        "table_count = table_env.from_path('dev_catalog.nba.`salaries@dev`').select('Season.count').to_pandas().values[0][0]\n",
        "csv_count = table_env.from_path('dev_catalog.nba.salaries_temp').select('Season.count').to_pandas().values[0][0]\n",
        "assert table_count == csv_count\n",
        "print(table_count)\n",
        "\n",
        "table_count = table_env.from_path('dev_catalog.nba.`totals_stats@dev`').select('Season.count').to_pandas().values[0][0]\n",
        "csv_count = table_env.from_path('dev_catalog.nba.totals_stats_temp').select('Season.count').to_pandas().values[0][0]\n",
        "assert table_count == csv_count\n",
        "print(table_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check generated tables\n",
        "----------------------------\n",
        "Since we have been working solely on the `dev` branch, where we created 2 tables and added some data,\n",
        "let's verify that the `main` branch was not altered by our changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n"
        }
      ],
      "source": [
        "!nessie contents --list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "And on the `dev` branch we expect to see two tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "ICEBERG_TABLE:\n\tnba.salaries\n\tnba.totals_stats\n\n"
        }
      ],
      "source": [
        "!nessie contents --list --ref dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We can also verify that the `dev` and `main` branches point to different commits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\u001b[33m* main  2e1cfa82b035c26cbbbdae632cea070514eb8b773f616aaeaf668e2f0be8f10d comment\n\u001b[0m  dev   3f743d120c8b4898237b19f7e82d3193bea7c5cfb6e1ffb4b8671de548f70422 comment\n\n"
        }
      ],
      "source": [
        "!nessie --verbose branch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dev promotion into main\n",
        "-----------------------\n",
        "Once we are done with our changes on the `dev` branch, we would like to merge those changes into `main`.\n",
        "We merge `dev` into `main` via the command line `merge` command.\n",
        "Both branches should be at the same revision after merging/promotion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n"
        }
      ],
      "source": [
        "!nessie merge dev -b main --force"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We can verify the branches are at the same hash and that the `main` branch now contains the expected tables and row counts.\n",
        "\n",
        "The tables are now on `main` and ready for consumtion by our blog authors and analysts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\u001b[33m* main  3f743d120c8b4898237b19f7e82d3193bea7c5cfb6e1ffb4b8671de548f70422 comment\n\u001b[0m  dev   3f743d120c8b4898237b19f7e82d3193bea7c5cfb6e1ffb4b8671de548f70422 comment\n\n"
        }
      ],
      "source": [
        "!nessie --verbose branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "ICEBERG_TABLE:\n\tnba.salaries\n\tnba.totals_stats\n\n"
        }
      ],
      "source": [
        "!nessie contents --list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2022-03-10 17:08:21,338 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:21,432 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:21,435 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:21,437 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:21,440 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:21,444 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:21,448 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:21,451 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:24,106 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:24,209 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:24,215 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:24,221 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:24,226 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:24,232 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:24,238 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:24,242 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n"
        }
      ],
      "source": [
        "table_count = table_env.from_path('main_catalog.nba.salaries').select('Season.count').to_pandas().values[0][0]\n",
        "csv_count = table_env.from_path('dev_catalog.nba.salaries_temp').select('Season.count').to_pandas().values[0][0]\n",
        "assert table_count == csv_count\n",
        "\n",
        "table_count = table_env.from_path('main_catalog.nba.totals_stats').select('Season.count').to_pandas().values[0][0]\n",
        "csv_count = table_env.from_path('dev_catalog.nba.totals_stats_temp').select('Season.count').to_pandas().values[0][0]\n",
        "assert table_count == csv_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Perform regular ETL on the new tables\n",
        "-------------------\n",
        "Our analysts are happy with the data and we want to now regularly ingest data to keep things up to date. Our first ETL job consists of the following:\n",
        "\n",
        "1. Update the salaries table to add new data\n",
        "2. We have decided the `Age` column isn't required in the `totals_stats` table so we will drop the column\n",
        "3. We create a new table to hold information about the players appearances in all star games\n",
        "\n",
        "As always we will do this work on a branch and verify the results. This ETL job can then be set up to run nightly with new stats and salary information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "create_ref_catalog(\"etl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2022-03-10 17:08:25,838 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:25,838 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:25,838 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:25,838 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:25,838 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:25,838 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:25,839 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:25,839 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n"
        }
      ],
      "source": [
        "# add some salaries for Kevin Durant\n",
        "table_env.execute_sql(\"\"\"INSERT INTO etl_catalog.nba.salaries\n",
        "                        VALUES ('2017-18', 'Golden State Warriors', '$25000000', 'Kevin Durant'),\n",
        "                        ('2018-19', 'Golden State Warriors', '$30000000', 'Kevin Durant'),\n",
        "                        ('2019-20', 'Brooklyn Nets', '$37199000', 'Kevin Durant'),\n",
        "                        ('2020-21', 'Brooklyn Nets', '$39058950', 'Kevin Durant')\"\"\").wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Rename the table `totals_stats` to `new_totals_stats`\n",
        "table_env.execute_sql(\"ALTER TABLE etl_catalog.nba.totals_stats RENAME TO etl_catalog.nba.new_totals_stats\").wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2022-03-10 17:08:26,962 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:26,962 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:26,962 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:26,962 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:26,962 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:26,963 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:26,963 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:26,963 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:28,763 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:28,870 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:28,876 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:28,881 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:28,886 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:28,892 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:28,897 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:28,902 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n"
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>Age</th>\n      <th>Team</th>\n      <th>ORB</th>\n      <th>TRB</th>\n      <th>AST</th>\n      <th>STL</th>\n      <th>BLK</th>\n      <th>TOV</th>\n      <th>PF</th>\n      <th>PTS</th>\n      <th>Player</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-10</td>\n      <td>25</td>\n      <td>CLE</td>\n      <td>1</td>\n      <td>5</td>\n      <td>6</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>25</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-11</td>\n      <td>26</td>\n      <td>MIA</td>\n      <td>2</td>\n      <td>12</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>29</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-12</td>\n      <td>27</td>\n      <td>MIA</td>\n      <td>0</td>\n      <td>6</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>36</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012-13</td>\n      <td>28</td>\n      <td>MIA</td>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>19</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2013-14</td>\n      <td>29</td>\n      <td>MIA</td>\n      <td>1</td>\n      <td>7</td>\n      <td>7</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>22</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2014-15</td>\n      <td>30</td>\n      <td>CLE</td>\n      <td>1</td>\n      <td>5</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>30</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2004-05</td>\n      <td>26</td>\n      <td>LAL</td>\n      <td>3</td>\n      <td>6</td>\n      <td>7</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5</td>\n      <td>16</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2005-06</td>\n      <td>27</td>\n      <td>LAL</td>\n      <td>0</td>\n      <td>7</td>\n      <td>8</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>8</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2006-07</td>\n      <td>28</td>\n      <td>LAL</td>\n      <td>1</td>\n      <td>5</td>\n      <td>6</td>\n      <td>6</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>31</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2007-08</td>\n      <td>29</td>\n      <td>LAL</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2008-09</td>\n      <td>30</td>\n      <td>LAL</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>27</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2009-10</td>\n      <td>31</td>\n      <td>LAL</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2010-11</td>\n      <td>32</td>\n      <td>LAL</td>\n      <td>10</td>\n      <td>14</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>37</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2011-12</td>\n      <td>33</td>\n      <td>LAL</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>27</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2012-13</td>\n      <td>34</td>\n      <td>LAL</td>\n      <td>2</td>\n      <td>4</td>\n      <td>8</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>9</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2013-14</td>\n      <td>35</td>\n      <td>LAL</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2014-15</td>\n      <td>36</td>\n      <td>LAL</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2015-16</td>\n      <td>37</td>\n      <td>LAL</td>\n      <td>1</td>\n      <td>6</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Season</td>\n      <td>Age</td>\n      <td>Team</td>\n      <td>ORB</td>\n      <td>TRB</td>\n      <td>AST</td>\n      <td>STL</td>\n      <td>BLK</td>\n      <td>TOV</td>\n      <td>PF</td>\n      <td>PTS</td>\n      <td>Player</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2004-05</td>\n      <td>20</td>\n      <td>CLE</td>\n      <td>1</td>\n      <td>8</td>\n      <td>6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>13</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2005-06</td>\n      <td>21</td>\n      <td>CLE</td>\n      <td>2</td>\n      <td>6</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>29</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2006-07</td>\n      <td>22</td>\n      <td>CLE</td>\n      <td>0</td>\n      <td>6</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>28</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2007-08</td>\n      <td>23</td>\n      <td>CLE</td>\n      <td>1</td>\n      <td>8</td>\n      <td>9</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>27</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2008-09</td>\n      <td>24</td>\n      <td>CLE</td>\n      <td>0</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>20</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1986-87</td>\n      <td>23</td>\n      <td>CHI</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>11</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1987-88</td>\n      <td>24</td>\n      <td>CHI</td>\n      <td>3</td>\n      <td>8</td>\n      <td>3</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>5</td>\n      <td>40</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1988-89</td>\n      <td>25</td>\n      <td>CHI</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>5</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>28</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1989-90</td>\n      <td>26</td>\n      <td>CHI</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>17</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1990-91</td>\n      <td>27</td>\n      <td>CHI</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>10</td>\n      <td>2</td>\n      <td>26</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1991-92</td>\n      <td>28</td>\n      <td>CHI</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>18</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>1997-98</td>\n      <td>19</td>\n      <td>LAL</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>18</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>1999-00</td>\n      <td>21</td>\n      <td>LAL</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>2000-01</td>\n      <td>22</td>\n      <td>LAL</td>\n      <td>2</td>\n      <td>4</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>19</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>2001-02</td>\n      <td>23</td>\n      <td>LAL</td>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>31</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>2002-03</td>\n      <td>24</td>\n      <td>LAL</td>\n      <td>2</td>\n      <td>7</td>\n      <td>6</td>\n      <td>3</td>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>22</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>2003-04</td>\n      <td>25</td>\n      <td>LAL</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>6</td>\n      <td>3</td>\n      <td>20</td>\n      <td>Kobe Bryant</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>2015-16</td>\n      <td>31</td>\n      <td>CLE</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>13</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>2016-17</td>\n      <td>32</td>\n      <td>CLE</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>23</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>2017-18</td>\n      <td>33</td>\n      <td>CLE</td>\n      <td>0</td>\n      <td>10</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>29</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>2018-19</td>\n      <td>34</td>\n      <td>LAL</td>\n      <td>2</td>\n      <td>8</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>19</td>\n      <td>Lebron James</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>1984-85</td>\n      <td>21</td>\n      <td>CHI</td>\n      <td>3</td>\n      <td>6</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>7</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>1985-86</td>\n      <td>22</td>\n      <td>CHI</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>1992-93</td>\n      <td>29</td>\n      <td>CHI</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>4</td>\n      <td>0</td>\n      <td>6</td>\n      <td>5</td>\n      <td>30</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>1995-96</td>\n      <td>32</td>\n      <td>CHI</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>20</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>1996-97</td>\n      <td>33</td>\n      <td>CHI</td>\n      <td>3</td>\n      <td>11</td>\n      <td>11</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>4</td>\n      <td>14</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>1997-98</td>\n      <td>34</td>\n      <td>CHI</td>\n      <td>1</td>\n      <td>6</td>\n      <td>8</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>23</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>2001-02</td>\n      <td>38</td>\n      <td>WAS</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>8</td>\n      <td>Michael Jordan</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>2002-03</td>\n      <td>39</td>\n      <td>WAS</td>\n      <td>2</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>20</td>\n      <td>Michael Jordan</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "     Season  Age  Team  ORB  TRB  AST  STL  BLK  TOV  PF  PTS          Player\n0   2009-10   25   CLE    1    5    6    4    0    2   1   25    Lebron James\n1   2010-11   26   MIA    2   12   10    0    0    4   3   29    Lebron James\n2   2011-12   27   MIA    0    6    7    0    0    4   2   36    Lebron James\n3   2012-13   28   MIA    0    3    5    1    0    4   0   19    Lebron James\n4   2013-14   29   MIA    1    7    7    3    0    1   0   22    Lebron James\n5   2014-15   30   CLE    1    5    7    2    0    4   1   30    Lebron James\n6   2004-05   26   LAL    3    6    7    3    1    4   5   16     Kobe Bryant\n7   2005-06   27   LAL    0    7    8    3    0    3   5    8     Kobe Bryant\n8   2006-07   28   LAL    1    5    6    6    0    4   1   31     Kobe Bryant\n9   2007-08   29   LAL    0    1    0    0    0    0   0    0     Kobe Bryant\n10  2008-09   30   LAL    1    4    4    4    0    1   0   27     Kobe Bryant\n11  2009-10   31   LAL                                            Kobe Bryant\n12  2010-11   32   LAL   10   14    3    3    0    4   2   37     Kobe Bryant\n13  2011-12   33   LAL    0    1    1    2    0    1   2   27     Kobe Bryant\n14  2012-13   34   LAL    2    4    8    2    2    1   2    9     Kobe Bryant\n15  2013-14   35   LAL                                            Kobe Bryant\n16  2014-15   36   LAL                                            Kobe Bryant\n17  2015-16   37   LAL    1    6    7    1    0    1   1   10     Kobe Bryant\n18   Season  Age  Team  ORB  TRB  AST  STL  BLK  TOV  PF  PTS          Player\n19  2004-05   20   CLE    1    8    6    2    0    3   0   13    Lebron James\n20  2005-06   21   CLE    2    6    2    2    0    1   2   29    Lebron James\n21  2006-07   22   CLE    0    6    6    1    0    4   0   28    Lebron James\n22  2007-08   23   CLE    1    8    9    2    2    4   3   27    Lebron James\n23  2008-09   24   CLE    0    5    3    0    0    3   0   20    Lebron James\n24  1986-87   23   CHI    0    0    4    2    0    5   2   11  Michael Jordan\n25  1987-88   24   CHI    3    8    3    4    4    2   5   40  Michael Jordan\n26  1988-89   25   CHI    1    2    3    5    0    4   1   28  Michael Jordan\n27  1989-90   26   CHI    1    5    2    5    1    5   1   17  Michael Jordan\n28  1990-91   27   CHI    3    5    5    2    0   10   2   26  Michael Jordan\n29  1991-92   28   CHI    1    1    5    2    0    1   2   18  Michael Jordan\n30  1997-98   19   LAL    2    6    1    2    0    1   1   18     Kobe Bryant\n31  1999-00   21   LAL    1    1    3    2    0    1   3   15     Kobe Bryant\n32  2000-01   22   LAL    2    4    7    1    0    3   3   19     Kobe Bryant\n33  2001-02   23   LAL    2    5    5    1    0    0   2   31     Kobe Bryant\n34  2002-03   24   LAL    2    7    6    3    2    5   5   22     Kobe Bryant\n35  2003-04   25   LAL    1    4    4    5    1    6   3   20     Kobe Bryant\n36  2015-16   31   CLE    0    4    7    0    0    4   0   13    Lebron James\n37  2016-17   32   CLE    0    3    1    0    0    4   2   23    Lebron James\n38  2017-18   33   CLE    0   10    8    1    0    5   2   29    Lebron James\n39  2018-19   34   LAL    2    8    4    0    2    1   1   19    Lebron James\n40  1984-85   21   CHI    3    6    2    3    1    1   4    7  Michael Jordan\n41  1985-86   22   CHI                                         Michael Jordan\n42  1992-93   29   CHI    3    4    5    4    0    6   5   30  Michael Jordan\n43  1995-96   32   CHI    1    4    1    1    0    0   1   20  Michael Jordan\n44  1996-97   33   CHI    3   11   11    2    0    3   4   14  Michael Jordan\n45  1997-98   34   CHI    1    6    8    3    0    2   0   23  Michael Jordan\n46  2001-02   38   WAS    0    4    3    2    0    1   1    8  Michael Jordan\n47  2002-03   39   WAS    2    5    2    2    0    2   3   20  Michael Jordan"
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating `allstar_games_stats` table\n",
        "(table_env.connect(FileSystem().path('../datasets/nba/allstar_games_stats.csv'))\n",
        "    .with_format(OldCsv()\n",
        "                 .field('Season', DataTypes.STRING()).field(\"Age\", DataTypes.STRING()).field(\"Team\", DataTypes.STRING())\n",
        "                 .field(\"ORB\", DataTypes.STRING()).field(\"TRB\", DataTypes.STRING()).field(\"AST\", DataTypes.STRING())\n",
        "                 .field(\"STL\", DataTypes.STRING()).field(\"BLK\", DataTypes.STRING()).field(\"TOV\", DataTypes.STRING())\n",
        "                 .field(\"PF\", DataTypes.STRING()).field(\"PTS\", DataTypes.STRING()).field(\"Player\", DataTypes.STRING()))\n",
        "    .with_schema(Schema()\n",
        "                 .field('Season', DataTypes.STRING()).field(\"Age\", DataTypes.STRING()).field(\"Team\", DataTypes.STRING())\n",
        "                 .field(\"ORB\", DataTypes.STRING()).field(\"TRB\", DataTypes.STRING()).field(\"AST\", DataTypes.STRING())\n",
        "                 .field(\"STL\", DataTypes.STRING()).field(\"BLK\", DataTypes.STRING()).field(\"TOV\", DataTypes.STRING())\n",
        "                 .field(\"PF\", DataTypes.STRING()).field(\"PTS\", DataTypes.STRING()).field(\"Player\", DataTypes.STRING()))\n",
        "    .create_temporary_table('etl_catalog.nba.allstar_games_stats_temp'))\n",
        "\n",
        "table_env.execute_sql(\n",
        "        \"\"\"CREATE TABLE IF NOT EXISTS etl_catalog.nba.allstar_games_stats (Season STRING, Age STRING,\n",
        "        Team STRING, ORB STRING, TRB STRING, AST STRING, STL STRING, BLK STRING, TOV STRING,\n",
        "        PF STRING, PTS STRING, Player STRING)\"\"\").wait()\n",
        "\n",
        "tab = table_env.from_path('etl_catalog.nba.allstar_games_stats_temp')\n",
        "tab.execute_insert('etl_catalog.nba.allstar_games_stats').wait()\n",
        "\n",
        "# Notice how we view the data on the etl branch via @etl\n",
        "table_env.from_path('etl_catalog.nba.`allstar_games_stats@etl`').to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can verify that the new table isn't on the `main` branch but is present on the etl branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "ICEBERG_TABLE:\n\tnba.salaries\n\tnba.totals_stats\n\n"
        }
      ],
      "source": [
        "# Since we have been working on the `etl` branch, the `allstar_games_stats` table is not on the `main` branch\n",
        "!nessie contents --list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "ICEBERG_TABLE:\n\tnba.salaries\n\tnba.allstar_games_stats\n\tnba.new_totals_stats\n\n"
        }
      ],
      "source": [
        "# We should see `allstar_games_stats` and the `new_totals_stats` on the `etl` branch\n",
        "!nessie contents --list --ref etl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we are happy with the data we can again merge it into `main`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n"
        }
      ],
      "source": [
        "!nessie merge etl -b main --force"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now lets verify that the changes exist on the `main` branch and that the `main` and `etl` branches have the same `hash`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "ICEBERG_TABLE:\n\tnba.salaries\n\tnba.allstar_games_stats\n\tnba.new_totals_stats\n\n"
        }
      ],
      "source": [
        "!nessie contents --list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "  etl   c410d11b2f4c57e5d649d405d46b19d62c389c9837103c44fb2da35ddff835d9 comment\n\u001b[33m* main  c410d11b2f4c57e5d649d405d46b19d62c389c9837103c44fb2da35ddff835d9 comment\n\u001b[0m  dev   3f743d120c8b4898237b19f7e82d3193bea7c5cfb6e1ffb4b8671de548f70422 comment\n\n"
        }
      ],
      "source": [
        "!nessie --verbose branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2022-03-10 17:08:33,686 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:33,769 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:33,773 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:33,777 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:33,782 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:33,786 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:33,790 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n2022-03-10 17:08:33,794 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.gz]\n"
        }
      ],
      "source": [
        "table_count = table_env.from_path('main_catalog.nba.allstar_games_stats').select('Season.count').to_pandas().values[0][0]\n",
        "csv_count = table_env.from_path('etl_catalog.nba.allstar_games_stats_temp').select('Season.count').to_pandas().values[0][0]\n",
        "assert table_count == csv_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create `experiment` branch\n",
        "--------------------------------\n",
        "As a data analyst we might want to carry out some experiments with some data, without affecting `main` in any way.\n",
        "As in the previous examples, we can just get started by creating an `experiment` branch off of `main`\n",
        "and carry out our experiment, which could consist of the following steps:\n",
        "- drop `totals_stats` table\n",
        "- add data to `salaries` table\n",
        "- compare `experiment` and `main` tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "create_ref_catalog(\"experiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": "<pyflink.table.table_result.TableResult at 0x7fce02a43e50>"
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Drop the `new_totals_stats` table on the `experiment` branch\n",
        "table_env.execute_sql(\"DROP TABLE experiment_catalog.nba.new_totals_stats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2022-03-10 17:08:35,088 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:35,088 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:35,088 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:35,088 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:35,088 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:35,088 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:35,088 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n2022-03-10 17:08:35,089 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.gz]\n"
        }
      ],
      "source": [
        "# add some salaries for Dirk Nowitzki\n",
        "table_env.execute_sql(\"\"\"INSERT INTO experiment_catalog.nba.salaries VALUES\n",
        "    ('2015-16', 'Dallas Mavericks', '$8333333', 'Dirk Nowitzki'),\n",
        "    ('2016-17', 'Dallas Mavericks', '$25000000', 'Dirk Nowitzki'),\n",
        "    ('2017-18', 'Dallas Mavericks', '$5000000', 'Dirk Nowitzki'),\n",
        "    ('2018-19', 'Dallas Mavericks', '$5000000', 'Dirk Nowitzki')\"\"\").wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "ICEBERG_TABLE:\n\tnba.salaries\n\tnba.allstar_games_stats\n\n"
        }
      ],
      "source": [
        "# We should see the `salaries` and `allstar_games_stats` tables only (since we just dropped `new_totals_stats`)\n",
        "!nessie contents --list --ref experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "ICEBERG_TABLE:\n\tnba.salaries\n\tnba.allstar_games_stats\n\tnba.new_totals_stats\n\n"
        }
      ],
      "source": [
        "# `main` hasn't changed been changed and still has the `new_totals_stats` table\n",
        "!nessie contents --list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at the contents of the `salaries` table on the `experiment` branch.\n",
        "Notice the use of the `nessie` catalog and the use of `@experiment` to view data on the `experiment` branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EXPR$0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   EXPR$0\n0      59"
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table_env.from_path('main_catalog.nba.`salaries@experiment`').select(lit(1).count).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and compare to the contents of the `salaries` table on the `main` branch. Notice that we didn't have to specify `@branchName` as it defaulted\n",
        "to the `main` branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EXPR$0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>55</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   EXPR$0\n0      55"
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table_env.from_path('main_catalog.nba.`salaries@main`').select(lit(1).count).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "flink-demo",
      "language": "python",
      "name": "flink-demo"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}