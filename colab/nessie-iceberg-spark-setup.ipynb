{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Nessie Iceberg Spark Setup\n",
    "\n",
    "This demo showcases how to use setup Apache Spark + Apache Iceberg for Nessie using Python.\n",
    "\n",
    "## Initialize Pyspark + Nessie environment\n",
    "\n",
    "To get started, we will first have to do a few setup steps that give us everything we need\n",
    "to get started with Nessie. The `nessiedemo` lib is used to start a Nessie server for this demo.\n",
    "In case you're interested in the detailed setup steps for Spark, you can check out the [docs](https://projectnessie.org/tools/spark/)\n",
    "or also directly have a look into the source code of the `nessiedemo` lib [here](https://github.com/projectnessie/nessie-demos/blob/main/pydemolib/nessiedemo/iceberg_spark.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# install the nessiedemo lib, which configures all required dependencies\n",
    "!pip install nessiedemo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setup the Demo: installs the required Python dependencies, downloads the sample datasets and\n",
    "# downloads + starts the Nessie-Quarkus-Runner.\n",
    "from nessiedemo.demo import setup_demo\n",
    "demo = setup_demo(\"nessie-0.5-iceberg-0.11.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The above started the Nessie server for us and also installed the Nessie CLI.\n",
    "!nessie branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Pyspark with Iceberg\n",
    "\n",
    "We have a running Nessie server.\n",
    "\n",
    "To use Iceberg + Spark with Nessie, the following components are needed. Those will be installed in the following steps.\n",
    "1. Pyspark, the Python Spark library\n",
    "1. Spark distribution (it's included in the `pyspark` library), so we do not need to download it separately\n",
    "1. Iceberg-Spark, which will be installed by the Spark runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Install `pyspark`\n",
    "\n",
    "The `nessiedemo` lib already has installed `pyspark` for us, as shown below. Please take care to use the same release\n",
    "version for the Apache Spark distribution from the [Spark Download page](https://spark.apache.org/downloads.html)\n",
    "and the `pyspark` library.\n",
    "\n",
    "Installing `pyspark` in a production environment is performed using `pip` with a requirement like `pyspark==3.0.2`,\n",
    "for example `pip install pyspark==3.0.2`. The exact version depends on the actual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip show pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Install Spark distribution\n",
    "\n",
    "There are different approaches to setup the Spark distribution.\n",
    "1. Use the Spark distribution that comes with the `pyspark` library\n",
    "1. Download the Spark distribution from the [Spark Download page](https://spark.apache.org/downloads.html), choose the\n",
    "   Spark release that matches the `pyspark` library. In this demo, we will use Spark \"Pre-built for Apache Hadoop 2.7\".\n",
    "   In a production environment, take care to use the Spark distribution for the Hadoop version you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Alternative 1: Using Spark from `pyspark`\n",
    "\n",
    "The above `pip show pyspark` gives us the path to the library in the line starting with `Location:`.\n",
    "It usually ends with `site-packages` or `dist-packages`. You need to add `/pyspark` to the path shown in the line\n",
    "starting with `Location:` to get the full path.\n",
    "\n",
    "The following Python code should give us the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "\n",
    "# Search for `pyspark` in Python package installation directories\n",
    "spark_dir = None\n",
    "for dir in site.getsitepackages():\n",
    "    test_dir = os.path.join(dir, \"pyspark\")\n",
    "    if os.path.isdir(test_dir):\n",
    "        spark_dir = test_dir\n",
    "        break\n",
    "\n",
    "if not spark_dir:\n",
    "    raise Exception(\"No pyspark package installed\")\n",
    "\n",
    "print(f\"Found Spark distribution in {spark_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Alternative 2: Download Spark\n",
    "\n",
    "This will download the Spark distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The `nessiedemo` library provides the URL to download the Spark distribution.\n",
    "spark_download_url = demo._get_versions_dict()[\"spark\"][\"tarball\"]\n",
    "\n",
    "print(f\"Spark distribution download URL is {spark_download_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the Spark directory name from the download URL - so we have something like\n",
    "# 'spark-3.0.2-bin-hadoop2.7' in spark_dir_name and\n",
    "# 'spark-3.0.2-bin-hadoop2.7.tgz' in spark_file_name and\n",
    "import re\n",
    "spark_dir_name = re.match(\".*[/]([a-zA-Z0-9-.]+)[.]tgz\", spark_download_url).group(1)\n",
    "spark_file_name = f\"{spark_dir_name}.tgz\"\n",
    "\n",
    "# Now download the Spark distribution\n",
    "from nessiedemo.demo import _Util\n",
    "_Util.wget(spark_download_url, spark_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Look for the downloaded tarball\n",
    "!ls -al ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the downloaded tarball\n",
    "_Util.exec_fail([\"tar\", \"-x\", \"-f\", spark_file_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Look for the Spark distribution directory\n",
    "!ls -al ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set the `spark_dir` variable\n",
    "spark_dir = os.path.abspath(spark_dir_name)\n",
    "\n",
    "print(f\"Extracted Spark distribution in {spark_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Common for both alternatives\n",
    "\n",
    "Once we have the Spark distribution handy and its path in `spark_dir`, just set the `SPARK_HOME` environment variable\n",
    "and use `findspark` to wire it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the SPARK_HOME environment variable\n",
    "os.environ[\"SPARK_HOME\"] = spark_dir\n",
    "\n",
    "print(os.environ[\"SPARK_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, use the Python `findspark` Package to \"wire\" it up.\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create a `SparkSession` to use Iceberg and Nessie\n",
    "\n",
    "Creating a `SparkSession` is basically two steps:\n",
    "1. Gather the configuration options and put those into a `SparkConf` instance\n",
    "1. Create the `SparkSession` instance using that `SparkConf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gather configuration in `SparkConf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the Iceberg version\n",
    "iceberg_version = demo.get_iceberg_version()\n",
    "\n",
    "print(f\"Using Iceberg version {iceberg_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are using the Spark configuration option `spark.jars.packages`\n",
    "(see [Spark Docs](https://spark.apache.org/docs/latest/configuration.html) for details) to let Spark pull the\n",
    "Iceberg Spark runtime. This option takes so called Maven coordinates, which we will prepare in the `spark_jars`\n",
    "variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark_jars = f\"org.apache.iceberg:iceberg-spark3-runtime:{iceberg_version}\"\n",
    "\n",
    "print(spark_jars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We also need the Nessie server's API endpoint URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the Nessie server's API endpoint URI\n",
    "nessie_api_uri = demo.get_nessie_api_uri()\n",
    "\n",
    "print(nessie_api_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We need a name for our catalog as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"iceberg_spark_setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we use the local filesystem for our \"warehouse\", just put that into some directory on the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark_warehouse = os.path.abspath(\"spark_warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.set(\"spark.jars.packages\", spark_jars)\n",
    "conf.set(\"spark.sql.execution.pyarrow.enabled\", \"true\")\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.warehouse\", spark_warehouse)\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.cache-enabled\", \"false\")\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "\n",
    "# Nessie specific configuration\n",
    "\n",
    "# The Nessie API endpoint\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.url\", nessie_api_uri)\n",
    "# Use the default branch called `main`\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.ref\", \"main\")\n",
    "# Tell Iceberg to use the Nessie catalog implementation\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "# Don't use authentication in this example\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.auth_type\", \"NONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create the `SparkSession`\n",
    "\n",
    "The next step creates the `SparkSession`.\n",
    "\n",
    "Note: If this step errors out with a message like \"Java Gateway process exited\", it probably means that you are running\n",
    "the demo on your local machine and the `JAVA_HOME` environment is not set. In that case, make sure you have Java 8 or\n",
    "Java 11 installed and `JAVA_HOME` set using `os.environ[\"JAVA_HOME\"] = <path to java-home>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `SparkSession` is created and ready to be used with Nessie.\n",
    "\n",
    "Look into other Nessie demos about how to use Nessie with Iceberg and Spark. This demo is just about the \"boilerplate\n",
    "initialization and setup code\". Other demos hide those parts using the `nessiedemo` library.\n",
    "\n",
    "The following snippets illustrate a few more topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Bonus content\n",
    "\n",
    "There are a few things that are probably worth to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Nessie references and Spark SQL with Iceberg\n",
    "\n",
    "Using multiple Nessie branches or tags in Spark SQL is easy, only add something like `@my_branch_name` to the\n",
    "table name.\n",
    "\n",
    "Some Spark SQL examples:\n",
    "\n",
    "| SQL | Explanation\n",
    "| --- | ---\n",
    "| ```SELECT * FROM my_table``` | Performs a `SELECT *` against the `my_table` table using the Nessie branch or tag from the `.ref` option in `SparkConf`.\n",
    "| ```SELECT * FROM `my_table@dev_branch` ``` | Performs a `SELECT *` against the `my_table` table but using the Nessie branch `dev_branch`. Note the backticks (``` ` ```) around the table qualifier.\n",
    "| ```SELECT * FROM `my_table@dev_branch` from_dev, `my_table@main` from_main WHERE ...``` | Performs a `SELECT` joining the `my_table` in the Nessie branch `dev_branch` with the `my_table` in the Nessie branch `main`, which can be handy to find differences in a table in different Nessie branches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Switching the `SparkSession` to another branch\n",
    "\n",
    "With Spark 3, you can create a new `SparkSession` and just set the `.ref` configuration option to point it to the\n",
    "Nessie branch or tag you like to use. In the following example, `spark_dev` will point to the Nessie `dev` branch.\n",
    "\n",
    "Note: Spark has a few \"static\" (\"thread local\") pointers. One is a Java `ThreadLocal` that holds the current\n",
    "`SparkSession`. If you want to use a different `SparkSession`, you have to call `SparkSession.setActiveSession(newSession)`\n",
    "to inform Spark about the \"right\" `SparkSession`. It is _not_ sufficient to \"just use\" the \"right\" `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the dev branch\n",
    "!nessie branch dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List the branches\n",
    "!nessie branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark_dev = spark.newSession()\n",
    "spark_dev.conf.set(f\"spark.sql.catalog.{catalog_name}.ref\", \"dev\")\n",
    "\n",
    "from py4j.java_gateway import java_import\n",
    "# Get the JVM (Java Virtual Machine) gateway used by pyspark\n",
    "jvm = spark.sparkContext._gateway.jvm\n",
    "java_import(jvm, \"org.apache.spark.sql.SparkSession\")\n",
    "\n",
    "# This step instructs Spark to use `spark_dev` for the current thread.\n",
    "jvm.SparkSession.setActiveSession(spark_dev._jsparkSession)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
