{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Nessie Iceberg Spark Setup\n",
    "\n",
    "This demo showcases how to use setup Apache Spark + Apache Iceberg for Nessie using Python.\n",
    "\n",
    "## Initialize Pyspark + Nessie environment\n",
    "\n",
    "To get started, we will first have to do a few setup steps that give us everything we need\n",
    "to get started with Nessie. The `nessiedemo` lib is used to start a Nessie server for this demo.\n",
    "In case you're interested in the detailed setup steps for Spark, you can check out the [docs](https://projectnessie.org/tools/spark/)\n",
    "or also directly have a look into the source code of the `nessiedemo` lib [here](https://github.com/projectnessie/nessie-demos/blob/main/pydemolib/nessiedemo/spark.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# install the nessiedemo lib, which configures all required dependencies\n",
    "!pip install nessiedemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setup the Demo: installs the required Python dependencies, downloads the sample datasets and\n",
    "# downloads + starts the Nessie-Quarkus-Runner.\n",
    "from nessiedemo.demo import setup_demo\n",
    "demo = setup_demo(\"nessie-0.5-iceberg-0.11.yml\")\n",
    "\n",
    "# This is separate, because NessieDemo.prepare() via .start() implicitly installs the required dependencies.\n",
    "# Downloads and sets up Spark\n",
    "from nessiedemo.spark_base import NessieDemoSparkSupport\n",
    "helper = NessieDemoSparkSupport(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The above started the Nessie server for us and also installed the Nessie CLI.\n",
    "!nessie branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create a `SparkSession` to use Iceberg and Nessie\n",
    "\n",
    "Creating a `SparkSession` is basically two steps:\n",
    "1. Gather the configuration options and put those into a `SparkConf` instance\n",
    "1. Create the `SparkSession` instance using that `SparkConf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gather configuration in `SparkConf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the Iceberg version\n",
    "iceberg_version = demo.get_iceberg_version()\n",
    "\n",
    "print(f\"Using Iceberg version {iceberg_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are using the Spark configuration option `spark.jars.packages`\n",
    "(see [Spark Docs](https://spark.apache.org/docs/latest/configuration.html) for details) to let Spark pull the\n",
    "Iceberg Spark runtime. This option takes so called Maven coordinates, which we will prepare in the `spark_jars`\n",
    "variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark_jars = f\"org.apache.iceberg:iceberg-spark3-runtime:{iceberg_version}\"\n",
    "\n",
    "print(spark_jars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We also need the Nessie server's API endpoint URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the Nessie server's API endpoint URI\n",
    "nessie_api_uri = demo.get_nessie_api_uri()\n",
    "\n",
    "print(nessie_api_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We need a name for our catalog as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"iceberg_spark_setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we use the local filesystem for our \"warehouse\", just put that into some directory on the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "spark_warehouse = os.path.abspath(\"spark_warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.set(\"spark.jars.packages\", spark_jars)\n",
    "conf.set(\"spark.sql.execution.pyarrow.enabled\", \"true\")\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.warehouse\", spark_warehouse)\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.cache-enabled\", \"false\")\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "\n",
    "# Nessie specific configuration\n",
    "\n",
    "# The Nessie API endpoint\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.url\", nessie_api_uri)\n",
    "# Use the default branch called `main`\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.ref\", \"main\")\n",
    "# Tell Iceberg to use the Nessie catalog implementation\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "# Don't use authentication in this example\n",
    "conf.set(f\"spark.sql.catalog.{catalog_name}.auth_type\", \"NONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create the `SparkSession`\n",
    "\n",
    "The next step creates the `SparkSession`.\n",
    "\n",
    "Note: If this step errors out with a message like \"Java Gateway process exited\", it probably means that you are running\n",
    "the demo on your local machine and the `JAVA_HOME` environment is not set. In that case, make sure you have Java 8 or\n",
    "Java 11 installed and `JAVA_HOME` set using `os.environ[\"JAVA_HOME\"] = <path to java-home>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `SparkSession` is created and ready to be used with Nessie.\n",
    "\n",
    "Look into other Nessie demos about how to use Nessie with Iceberg and Spark. This demo is just about the \"boilerplate\n",
    "initialization and setup code\". Other demos hide those parts using the `nessiedemo` library.\n",
    "\n",
    "The following snippets illustrate a few more topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Bonus content\n",
    "\n",
    "There are a few things that are probably worth to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Nessie references and Spark SQL with Iceberg\n",
    "\n",
    "Using multiple Nessie branches or tags in Spark SQL is easy, only add something like `@my_branch_name` to the\n",
    "table name.\n",
    "\n",
    "Some Spark SQL examples:\n",
    "\n",
    "| SQL | Explanation\n",
    "| --- | ---\n",
    "| ```SELECT * FROM my_table``` | Performs a `SELECT *` against the `my_table` table using the Nessie branch or tag from the `.ref` option in `SparkConf`.\n",
    "| ```SELECT * FROM `my_table@dev_branch` ``` | Performs a `SELECT *` against the `my_table` table but using the Nessie branch `dev_branch`. Note the backticks (``` ` ```) around the table qualifier.\n",
    "| ```SELECT * FROM `my_table@dev_branch` from_dev, `my_table@main` from_main WHERE ...``` | Performs a `SELECT` joining the `my_table` in the Nessie branch `dev_branch` with the `my_table` in the Nessie branch `main`, which can be handy to find differences in a table in different Nessie branches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Switching the `SparkSession` to another branch\n",
    "\n",
    "With Spark 3, you can create a new `SparkSession` and just set the `.ref` configuration option to point it to the\n",
    "Nessie branch or tag you like to use. In the following example, `spark_dev` will point to the Nessie `dev` branch.\n",
    "\n",
    "Note: Spark has a few \"static\" (\"thread local\") pointers. One is a Java `ThreadLocal` that holds the current\n",
    "`SparkSession`. If you want to use a different `SparkSession`, you have to call `SparkSession.setActiveSession(newSession)`\n",
    "to inform Spark about the \"right\" `SparkSession`. It is _not_ sufficient to \"just use\" the \"right\" `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the dev branch\n",
    "!nessie branch dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List the branches\n",
    "!nessie branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark_dev = spark.newSession()\n",
    "spark_dev.conf.set(f\"spark.sql.catalog.{catalog_name}.ref\", \"dev\")\n",
    "\n",
    "from py4j.java_gateway import java_import\n",
    "# Get the JVM (Java Virtual Machine) gateway used by pyspark\n",
    "jvm = spark.sparkContext._gateway.jvm\n",
    "java_import(jvm, \"org.apache.spark.sql.SparkSession\")\n",
    "\n",
    "# This step instructs Spark to use `spark_dev` for the current thread.\n",
    "jvm.SparkSession.setActiveSession(spark_dev._jsparkSession)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}